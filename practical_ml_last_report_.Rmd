---
title: "Week4 peer review assignment."
author: "Lee"
date: "April 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Synopsis
This assignment is about predicting the physical activity by some data. classe feature of training data is the target that we're going to predict.    
This dataset has more than 100 features and some of them is meaningless such as X and user name and so on. I extracted numeric data because numeric data is more meaningful than others.   
At first, I tried randomforest of caret. But it didn't work. I found some solution about it in discussion board. But it still took so long. I decided to use randomForest package in direct.   
10 fold cross validation is implemented from the scratch. There must be a better way.But I wanted to experience it by myself.
The accuracy is so high. it's more than 99%.  

# Loading data & basic information
This dataset is not big. 
seed is not needed at the moment. It's added just for the future use.
```{r}
#library(caret)
library(randomForest)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

dim(training)
#head(training)

dim(testing)

set.seed(20170417)
```


# feature extraction
I only use numeric features. It was enough in order to get a high accuracy.   
numeric_only[1] is just an index. It doesn't say anything and have to be erased.
classe feature must be added because it's a target.
```{r}
numeric_only <- sapply(testing, is.numeric)
numeric_only[1] <- FALSE

essential <- training[,numeric_only]

essential$classe <- training$classe

```




# training using whole training dataset 
random forest module of caret package doesn't seem to work well.   
randomForest is used directly.  
Perfect score(20/20) in prediction quiz was gained.

```{r}
##it doesn't work...
##fit <- train(classe ~ . , data= essential ,method="rf")

model <- randomForest(classe ~. , data = essential)
out <- predict(model, newdata=testing[,numeric_only])
print(out)

```







# 10 fold cross validation.
essential is the feature extracted dataset from the upper code.  
"kfold" feature is added to identify which fold it is.
1:10 is repeated  (nrows - nrows%%10)/10 times. nrows - nrows%%10 calculate the rest. The second "for loop"" is for appending kfold feature value to the leftover.  
If kfold is 1 ,it means it's 1 fold. If kfold is 2, it means it's 2 fold and so on.   
grep function is for removing kfold becuase kfold is just added for identifying which fold it is.     
each accuracy is saved in result vector.   
correct_cnt can be obtained by comparing the prediction and the features classe(correction answer).   
The vector result is for stacking  accuracy of each iteration. This value is used for overall accuracy & sample error on the next code chunk.

```{r}


nrows <- nrow(essential)
fold_num <- rep(1:10 , times=(nrows - nrows%%10 )/10 )

for(i in range(1: (nrows%%10)))
{
  fold_num <- c(fold_num,i)
}

essential[,"kfold"] <- fold_num

result <- c()
for(i in 1:10)
{
  train_data <- essential[ essential[,"kfold"] != i , -grep("kfold", colnames(essential))]
  testing_data <-essential[ essential[,"kfold"] == i , -grep("kfold", colnames(essential))]

  ffit <- randomForest(classe ~ . , data = train_data )
  out <- predict(ffit, newdata=testing_data)
  
  #accuracy
  correct_cnt = sum( essential[ essential[,"kfold"] == i ,]$classe == out)
  result <- c(result, (correct_cnt /(nrows/10))*100 )
}


```


# accuracy & sample error
The accuracy of 10 fold cross validation will be mean of the vector "result".     
sample_error is just (100 - accuracy%).   

Therefore, accuracy is "99.94904%"" and sample error is "0.05096%".
```{r}
accuracy = sum(result)/10
print (accuracy)

sample_error = 100 - accuracy
print(sample_error)
```